{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59621233",
   "metadata": {},
   "source": [
    "## Finetuning Mistral на диалоговом датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1995e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f22787",
   "metadata": {},
   "source": [
    "**Загружаем датасет**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e156e4",
   "metadata": {},
   "source": [
    "**Получить бесплатный доступ к датасету Company Cases** очень просто:\n",
    "\n",
    "**1.** Подписываетесь на наши ресурсы: https://vk.com/pine_forest_ai, https://t.me/+Ml16EbQoepcwMGNi, https://www.linkedin.com/company/pineforest-ai/ и https://www.youtube.com/@PineForestAI.\n",
    "\n",
    "**2.** Подаете заявку в группу https://vk.com/adv_nlp_course, в которой находится датасет.\n",
    "\n",
    "**3.** Мы в течение нескольких часов принимаем заявку и вы сможете скачать в группе датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc25790",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"company_cases.json\", 'r') as inp:\n",
    "    raw_dataset = json.load(inp)\n",
    "\n",
    "train_raw_dataset, test_raw_dataset = train_test_split(raw_dataset)\n",
    "train_test_raw_dataset = {\"train\": train_raw_dataset, \"test\": test_raw_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cae49",
   "metadata": {},
   "source": [
    "**Преобразуем датасет в формат, который используется в классе Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_dict = {}\n",
    "for data_type in [\"train\", \"test\"]:\n",
    "    raw_dataset_dict[data_type] = {\n",
    "        \"instruction\": [element['instruction'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"personality\": [element['personality'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"context\": [element['context'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"dialog_start_line\": [element['instruction'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"dialog\": [element['dialog'] for element in train_test_raw_dataset[data_type]]\n",
    "    }\n",
    "\n",
    "train_dataset = Dataset.from_dict(raw_dataset_dict[\"train\"])\n",
    "test_dataset = Dataset.from_dict(raw_dataset_dict[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b160a2",
   "metadata": {},
   "source": [
    "**Функция для форматирования промпта**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c1580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    prompts = []\n",
    "    for i in range(len(examples['instruction'])):\n",
    "        prompt = f\"<s>system\\n{examples['instruction'][i]}\"\n",
    "        if examples['personality'][i]:\n",
    "            prompt += f\"\\n{examples['personality'][i]}\"\n",
    "        prompt += f\"\\n{examples['context'][i]}\"\n",
    "        if examples['dialog_start_line']:\n",
    "            prompt += f\"\\n{examples['dialog_start_line'][i]}\"\n",
    "        prompt += \"</s>\"\n",
    "        for element in examples['dialog'][i]:\n",
    "            prompt += f\"<s>{element['role']}\\n{element['content']}</s>\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31627ff",
   "metadata": {},
   "source": [
    "**Инициализируем токенизатор**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0884d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\")\n",
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653524e",
   "metadata": {},
   "source": [
    "Сделаем, чтобы **градиент при обучении протекал только через токены последней реплики чат-бота**, labels у остальных токенов сделаем равными -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6491fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"bot\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19178150",
   "metadata": {},
   "source": [
    "**Инициализируем модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c729815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Будем обучать модель в int4 для уменьшения требуемой видеопамяти\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Open-Orca/Mistral-7B-OpenOrca\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4', # квантизация модели в тип normal float 4\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc308d5",
   "metadata": {},
   "source": [
    "**Параметры адаптеров**: $r$ - размерность матрицы адаптеров, $lora\\_alpha$ - это значение, на которое после умножения вектора на матрицу-адаптер мы будем умножать каждую компоненту полученного вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd7f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    warmup_steps=100,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e70e05",
   "metadata": {},
   "source": [
    "**Инициализируем SFTTrainer и запускаем обучение.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98727197",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments,\n",
    "    max_seq_length=3700\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
