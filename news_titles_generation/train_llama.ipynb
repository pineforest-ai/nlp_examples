{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение LLaMa генерировать заголовки новостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<center>\n",
    "<img src=\"text_title_example.png\" width=\"900\"/>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Установим и импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_peft_model** - это функция, которая будет использоваться для добавления адаптеров по конфигурации в **LoraConfig** перед дообучением.\n",
    "**prepare_model_for_int8_training** - передаем модель в эту функцию, если хотим обучать модель в int8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Инициализация основной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим, на каком устройстве будем обучать модель (GPU или CPU).\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Название в Hugging Face Hub предобученной модели, чекпоинты которой мы будем использовать\n",
    "# Если используем LLaMa1\n",
    "#BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    "\n",
    "# Для LLaMa 2:\n",
    "# BASE_MODEL = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Если используем закрытую модель, такую как LLaMa 2:\n",
    "# В функцию from_pretrained нужно добавлять token от личного кабинета Hugging Face\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL, use_auth_token=token)\n",
    "\n",
    "# Мы уже скачали веса в локальную директорию torch_trnsf_models/llama-2-7b-hf:\n",
    "BASE_MODEL = \"torch_trnsf_models/llama-2-7b-hf\"\n",
    "\n",
    "# Инициализируем токенизатор для модели LLaMa\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# задаем токен для padding, то есть добавления в те последовательности из батча, которые короче,\n",
    "# чем максимальная длина последовательности, чтобы все последовательности в итоге были одной длины\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "# нули для padding будем добавлять слева\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем основную модель\n",
    "\"\"\"\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем основную модель\n",
    "# Если мы хотим обучать модель в int4 для уменьшения требуемой видеопамяти\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4', # квантизация модели в тип normal float 4\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число с плавающей точкой представляется в памяти в виде $(-1)^З * M * 2^{э - смещение}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<center>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/bitsandbytes/FP8-scheme.png\" width=\"900\"/>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сравнение потребления видеопамяти для разных конфигураций.**\n",
    "\n",
    "| Конфигурация | VRAM |\n",
    "| :--- | :--- |\n",
    "| 32bit | 24.8 Gb |\n",
    "| 8bit | 22Gb |\n",
    "| 4bit, BitsAndBytesConfig | 15.6 Gb |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Загрузка и предобработка данных\n",
    "\n",
    "Считываем датасет из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"news_titles_dataset.pickle\", 'rb') as inp:\n",
    "    dataset = pickle.load(inp)\n",
    "\n",
    "print(\"dataset length\", len(dataset))\n",
    "print(\"title:\", dataset[5][\"title\"])\n",
    "print(\"text:\", dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функция для получения строки с промптом** по инструкции, входному тексту и тексту, который должна сгенерировать модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"Сгенерируй заголовок к данному тексту.\"\n",
    "\n",
    "# Максимальная длина последовательности токенов на входе трансформера (если пос-ть длинее, обрезаем до CUTOFF_LEN)\n",
    "CUTOFF_LEN = 500\n",
    "\n",
    "def generate_prompt(sample):\n",
    "    # Также для разделения частей промпта можно использовать специальные токены начала и конца сегмента <s> и </s>\n",
    "    prompt = f\"{INSTRUCTION}\\nТекст:\\n{sample['text']}\\nЗаголовок: \"\n",
    "    full_prompt = f\"{INSTRUCTION}\\nТекст:\\n{sample['text']}\\nЗаголовок: {sample['title']}\"\n",
    "\n",
    "    # Если длина full_prompt больше, чем CUTOFF_LEN, удалим несколько последних предложений текста,\n",
    "    # пока длина не станет меньше, чем CUTOFF_LEN\n",
    "    if len(tokenizer(full_prompt)[\"input_ids\"]) > CUTOFF_LEN:\n",
    "        sentences = sample['text'].split(\". \")   # делим текст на предложения\n",
    "        while True:\n",
    "            sentences = sentences[:-1]\n",
    "            text = \". \".join(sentences)\n",
    "            prompt = f\"{INSTRUCTION}\\nТекст:\\n{text}\\nЗаголовок: \"\n",
    "            full_prompt = f\"{INSTRUCTION}\\nТекст:\\n{text}\\nЗаголовок: {sample['title']}\"\n",
    "            if len(tokenizer(full_prompt)[\"input_ids\"]) < CUTOFF_LEN:\n",
    "                break\n",
    "    return prompt, full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функция для токенизации промпта**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, full_prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        full_prompt,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "            and add_eos_token\n",
    "    ):\n",
    "        # если в конце пос-ти нет специального токена, мы его добавляем\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
    "    labels = result[\"input_ids\"].copy()\n",
    "    labels = [-100 for _ in range(prompt_len)] + labels[prompt_len:]\n",
    "    result[\"labels\"] = labels\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В функции **generate_and_tokenize_prompt** по примеру из датасета, переданному на вход, мы сначала получаем промпт, а потом его токенизируем. Дальше эту функцию применим для предобработки всех примеров из датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(sample):\n",
    "    prompt, full_prompt = generate_prompt(sample)\n",
    "    tokenized_full_prompt = tokenize(prompt, full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим выборку на обучающую и валидационную\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.1)\n",
    "\n",
    "# Предобработка данных (получение промпта для каждого примера из датасета и последующая токенизация)\n",
    "train_data = list(map(generate_and_tokenize_prompt, train_data))\n",
    "test_data = list(map(generate_and_tokenize_prompt, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator нужен для формирования батча (padding, сборка элементов батча в один тензор,\n",
    "# конвертация массивов numpy или списков в тензоры torch.LongTensor)\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True, label_pad_token_id=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Добавляем в LLaMa адаптеры\n",
    "\n",
    "Добавляем в трансформер (LLaMa), который будет заморожен во время обучения, обучаемые слои (адаптеры). То есть основные слои LLaMa останутся неизменные, обучаться будут только адаптеры. У добавляемых адаптеров значительно меньше параметров, чем у основной модели, за счет чего удается значительно сократить требуемый объем вычислительных ресурсов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<center>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png\" width=\"900\"/>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размерность матриц адаптеров\n",
    "# К примеру, если исходная матрица весов 4096 x 4096, то матрицы, которые мы добавляем,\n",
    "# имеют размерность 4096 х LORA_R и LORA_R х 4096.\n",
    "LORA_R = 8\n",
    "\n",
    "# После умножения на матрицу весов адаптеров компоненты вектора делим на LORA_R и умножаем на LORA_ALPHA\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# В какие слои трансформера будем добавлять адаптеры, в данном случае - в матрицы в слоях self-attention\n",
    "# для вычисления query и key.\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "# Делаем объект конфигурации по параметрам адаптеров\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Добавляем в трансформер адаптеры по параметрам, которые были переданы в LoraConfig.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)\n",
    "\n",
    "# Выведем информацию об обучаемых весах модели.\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Задаем гиперпараметры обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "TRAIN_EPOCHS = 3\n",
    "MICRO_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# директория, в которую будем сохранять чекпоинты модели\n",
    "OUTPUT_DIR = \"news_checkpoints\"\n",
    "\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=100,\n",
    "    max_steps=2000,\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "# компиляция модели (для оптимизации обучения)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраняем чекпоинты обученной модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
