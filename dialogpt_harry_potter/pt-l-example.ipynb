{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eae3f05",
   "metadata": {},
   "source": [
    "# Обучение dialogpt по Гарри Поттеру с помощью pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb4c787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /archive/evseev/envllm/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /archive/evseev/envllm/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/archive/evseev/envllm/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/lib/nvidia-384')}\n",
      "  warn(msg)\n",
      "/archive/evseev/envllm/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1148a8",
   "metadata": {},
   "source": [
    "### Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d2370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"harry_potter_dataset.json\", 'r') as inp:\n",
    "    dataset = json.load(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a1ee6",
   "metadata": {},
   "source": [
    "### Разбиение датасета на тренировочный и тестовый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22c3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "752bd231",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('pt-l-checkpoints'):\n",
    "    os.mkdir('pt-l-checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7c034",
   "metadata": {},
   "source": [
    "### Аргументы обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ab7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'pt-l-checkpoints'\n",
    "        self.pretrained_trf = 'microsoft/DialoGPT-small'\n",
    "        self.device = \"cuda\"\n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.per_gpu_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.01\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.num_train_epochs = 3\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 500\n",
    "        self.save_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a514d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65966d7",
   "metadata": {},
   "source": [
    "### Пользоветельский класс dataset'а"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2029b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, data: List[Tuple[List[str], str]], max_length: int = 512):\n",
    "        self.examples = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        dialogue = self.examples[item]\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "        conv = list([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in dialogue])\n",
    "        conv = flatten(conv)\n",
    "        conv = conv[-self.max_length:]\n",
    "        inputs = torch.tensor(conv, dtype=torch.long)\n",
    "        return {\"input_ids\": inputs, \"labels\": inputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b0787",
   "metadata": {},
   "source": [
    "### Функция для формирования батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed21ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(examples: List[torch.Tensor]):\n",
    "    ids = [example['input_ids'] for example in examples]\n",
    "\n",
    "    if tokenizer._pad_token is None:\n",
    "        padded = pad_sequence(ids, batch_first=True)\n",
    "    padded = pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return {\"input_ids\": padded, \"labels\": padded}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc7d1d",
   "metadata": {},
   "source": [
    "### Выбор девайса и вычисление размера батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38dbf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.device = torch.device(\"cuda\")\n",
    "    num_devices = torch.cuda.device_count()\n",
    "else:\n",
    "    args.device = torch.device(\"cpu\")\n",
    "\n",
    "args.train_batch_size = args.per_gpu_train_batch_size * num_devices\n",
    "args.eval_batch_size = args.per_gpu_eval_batch_size * num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d8a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.pretrained_trf)\n",
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028f2a0",
   "metadata": {},
   "source": [
    "### Загрузчики данных для train, eval и test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7409b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversarionDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_data, test_data, tokenizer, max_token_len=512):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ConversationDataset(\n",
    "            self.tokenizer,\n",
    "            self.train_data,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "        self.test_dataset = ConversationDataset(\n",
    "            self.tokenizer,\n",
    "            self.test_data,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=train_sampler, \n",
    "            batch_size=args.train_batch_size, \n",
    "            collate_fn=collate, \n",
    "            drop_last = True,\n",
    "            num_workers=40\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        eval_sampler = SequentialSampler(self.test_dataset)\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            sampler=eval_sampler, \n",
    "            batch_size=args.eval_batch_size, \n",
    "            collate_fn=collate, \n",
    "            drop_last = True,\n",
    "            num_workers=40\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        eval_sampler = SequentialSampler(self.test_dataset)\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            sampler=eval_sampler, \n",
    "            batch_size=args.eval_batch_size, \n",
    "            collate_fn=collate, \n",
    "            drop_last = True,\n",
    "            num_workers=40\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a120ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = ConversarionDataModule(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e359c1ee",
   "metadata": {},
   "source": [
    "### Класс с моделью и методами, используемыми при обучении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bc77dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelWithLMHead.from_pretrained(model_name)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, input_ids, labels):\n",
    "        output = self.model(input_ids,  return_dict=True, labels=labels)\n",
    "        return output['loss'], output['logits']\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.training_step_outputs.append(loss)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, labels)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, labels)\n",
    "        self.test_step_outputs.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        model_to_save = (\n",
    "            self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.training_step_outputs).mean()\n",
    "        print(\"----- training_epoch_average:\", epoch_average.item(), '-----')\n",
    "        self.log(\"training_epoch_average\", epoch_average.item())\n",
    "        self.training_step_outputs.clear()  # free memory\n",
    "                \n",
    "    def on_validation_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        print('===== val_loss_avg', epoch_average.item(), '='*5)\n",
    "        self.log(\"val_loss_avg\", epoch_average.item())\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "        \n",
    "    def on_test_epoch_end(self):\n",
    "        test_loss_avg = torch.stack(self.test_step_outputs).mean()\n",
    "        print('test_loss_avg', test_loss_avg.item())\n",
    "        self.test_step_outputs.clear()  # free memory\n",
    "        return test_loss_avg\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": args.weight_decay,\n",
    "            },\n",
    "            {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, \n",
    "                          lr=args.learning_rate, \n",
    "                          eps=args.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=args.warmup_steps, \n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "          optimizer=optimizer,\n",
    "          lr_scheduler=dict(\n",
    "            scheduler=scheduler,\n",
    "            interval='step'\n",
    "          )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe716b2",
   "metadata": {},
   "source": [
    "### Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6db8365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/archive/evseev/envllm/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1422: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = ConversationModel(\n",
    "    args.pretrained_trf,\n",
    "    n_warmup_steps=args.warmup_steps,\n",
    "    n_training_steps=len(train_data) // args.train_batch_size * args.num_train_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f84f1",
   "metadata": {},
   "source": [
    "### Настройки сохранения чекпоинтов и процесса обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e0a1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=args.output_dir,\n",
    "  save_top_k=2,\n",
    "  monitor=\"val_loss_avg\",\n",
    "  mode=\"min\",\n",
    "  every_n_train_steps=args.save_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e2b595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"pt_l_logs\", name=\"conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d2bb05a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    default_root_dir=args.output_dir,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_checkpointing=True,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    max_steps=args.max_steps,\n",
    "    devices=torch.cuda.device_count(),\n",
    "    log_every_n_steps=args.logging_steps,\n",
    "    val_check_interval=args.logging_steps,\n",
    "    enable_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8285f",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "079493e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/archive/evseev/envllm/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /archive/evseev/hp_train/pt-l-checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 124 M \n",
      "------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "497.759   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== val_loss_avg 7.254636764526367 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bfd4a42875458b908d83496a678079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/archive/evseev/envllm/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "784878d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8f5bc097b84bc38816de4fa5e5c497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss_avg 3.1944661140441895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0fa56",
   "metadata": {},
   "source": [
    "### Загрузка и запуск обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e003c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = ConversationModel.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path,\n",
    "    model_name = 'microsoft/DialoGPT-small'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e046364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/archive/evseev/envllm/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1422: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Harry, where is the Chamber of Secrets?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter Bot: I don't know.\n",
      ">> User:Who knows?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter Bot: We'll have to wait and see.\n",
      ">> User:May be Hermione knows?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter Bot: !!!!!!!!!!!!Harry!!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained('pt-l-checkpoints') # Let's chat for 3 lines\n",
    "\n",
    "for step in range(3):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids# generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"Harry Potter Bot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835019c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
